\documentclass[12pt]{article}
\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX

\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}

\parskip=2ex
\parindent=0ex
\textheight=270mm
\textwidth=7in
\oddsidemargin=-0.25in
\topmargin=-25mm
\pagestyle{empty}

\pagestyle{empty}
\newcommand{\bi}{\begin{itemize}\vspace*{-0.3in}}
\newcommand{\ei}{\end{itemize}\vspace*{-0.3in}}
\newcommand{\im}{\item\vspace*{-0.4in}}
\newcommand{\new}{^{\scriptscriptstyle \mathrm{new}}}
\newcommand{\bea}{\begin{eqnarray*}}
\newcommand{\eea}{\end{eqnarray*}}
\newcommand{\T}{{\scriptsize^{\top}}}
\newcommand{\yy}{{\mathbf y}}
\newcommand{\by}{{\mathbf y}}
\newcommand{\xx}{{\mathbf x}}
\newcommand{\bx}{{\mathbf x}}
\newcommand{\zz}{{\mathbf z}}
\newcommand{\Bet}{{\rm Beta}}
\newcommand{\deldel}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\hh}{\hspace{0.2in}}
\newcommand{\bpi}{{\boldsymbol{\pi}}}
\newcommand{\bmu}{{\boldsymbol{\mu}}}
\newcommand{\btheta}{{\boldsymbol{\theta}}} % amssymb
\newcommand{\blambda}{{\boldsymbol{\lambda}}} % amssymb
\newcommand{\bs}{{\mathbf s}}
\newcommand{\be}{\begin{equation*}}
\newcommand{\ee}{\end{equation*}}

\begin{document}

\centerline{\large\bf 4F13 Probabilistic Machine Learning: Coursework \#1: Gaussian Processes}

\vspace{0.3cm}

\centerline{Hong Ge} 

\vspace{0.2cm}

\centerline{Due (for non-MLMI students) 12:00 noon, Friday Nov 7th, 2025 online via moodle}

\vspace{2mm}

Your answers should contain an explanation of what you do, and max 2-4
central commands to achieve it (but complete listings are unnecessary
and discouraged) as part of the figures you provide. You must give an \emph{interpretation} of what the
numerical values and graphs you provide \emph{mean} -- why are the
results the way they are, and what are the consequences? Explain your
reasoning. {\bf Each question should be labelled and answered
  separately and distinctly.} All questions carry
approximately equal weight. 
Total combined length of answers must
not exceed 5 sides of A4 (plus cover page), minimum 11pt font, 1 inch
margins. 

You need the GPy package in Python, which can be easily installed using the following command in a terminal or command prompt: \texttt{pip install GPy}. Documentation for the package can be found at \url{https://gpy.readthedocs.io/en/deploy/}

\begin{enumerate}

\item[a)] Load data from \texttt{cw1a.mat} using \texttt{scipy.io.loadmat()}. Train a GP with a
  squared exponential covariance function, \texttt{GPy.kern.RBF()}. Initialise the hyperparameters (kernel lengthscale $\ell$, kernel variance $\sigma_f^2$, noise variance $\sigma_n^2$) as follows: $\ell = e^{-1}, \sigma_f^2=1, \sigma_n^2=1$. Minimise the negative log marginal likelihood. Show the 95\%
  predictive error bars. Explain the values of the optimised
  hyperparameters and the shape of the predictive error bars.

\item[b)] How can you find out whether the hyper parameter optimum is
  unique, or whether there may be other local optima? If there are
  local optima, find some, and explain what the model is doing in each
  case. Which fit is best, and why? Quantify how confident are you about this and why?

\item[c)] Train instead a GP with a periodic covariance
  function, \texttt{GPy.kern.StdPeriodic()}. Compare the behaviour of the error-bars with a). Do you
  think the data generating mechanism (apart from the noise) was
  strictly periodic\footnote{By strictly periodic, is meant a
    function where the exists a $p$ such that $f(x)=f(x+np)$ for
    integer $n$ and all
    $x$, not just a a function which ``goes up and down''.}? Carefully
  discuss the evindence for or against periodicity.  
  
\item[d)] Generate random (essentially) noise-free functions evaluated
  at \texttt{X = np.linspace(-5.0, 5.0, 200, dtype=np.float64).reshape(-1, 1)} from a GP with covariance function defined as the product of \texttt{GPy.kern.RBF} (with $\ell = e^2, \sigma_f^2 = 1.0$) and \texttt{GPy.kern.StdPeriodic} (with $\ell = e^{-0.5}, p=1.0, \sigma_f^2 = 1.0$). In order to apply the Cholesky
  decomposition to the covariance matrix, you may have to add a small
  diagonal matrix, for example \texttt{1e-6*np.eye(200)}, why?  Plot some
  sample functions. Carefully explain the relationship between the
  properties of those random functions and the form of the covariance
  function.

\item[e)] Load \texttt{cw1e.mat}. This data has 2-D input and
  scalar output. Visualise the data, e.g., using
  $${\small\texttt{plt.plot\_surface(X[:,0].reshape(11,11),
                 X[:,1].reshape(11,11),
                 y.reshape(11,11))}}.$$
  Rotate the data, to get a feel for it. Compare two GP models of the data: one with a single SE covariance \texttt{GPy.kern.RBF(input\_dim=2, ARD=True)}, and the other with a sum of two SE covariance functions \texttt{GPy.kern.RBF(input\_dim=2, ARD=True) + GPy.kern.RBF(input\_dim=2, ARD=True)}.
  For the second model, be sure to break symmetry with the initial hyperparameters (lengthscales and variance of each SE covariance), e.g., set them with \texttt{0.1*np.random.randn(6)}. 

  Compare the models: give a careful quantitative interpretation of
  the relationship between data fit, model complexity and marginal
  likelihood for each of the two models; which model is best and why,
  explain your reasoning.

\end{enumerate}

\end{document}